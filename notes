

Steps:
- get more data. run scraper. set up DB?
    - 
- run more models. make pipelines. try all methods
- word embeddings
-neural networks
-research for other types.

Things to do:

Do proper EDA
after scraping batches of 10, INSERT into db
put all the data in csv into database.
set up the database in the server
The number of the 


SCRAPER:

script to run scraper: indicate range of isbn to colelct on various sources: gbooks and greads
actually need to find all the shit and store in SQL.

Data cleaning
he 
Labelling: 
Rely on goodreads genres
Gbooks can be used to test data
Consolidate data


EDA: genres, top words in each genres

misclassified

Experiments:
- bag of words wordcount
- tfidf word count
- tfidf svm, NVB
-






check the set 2 csv for wrong column errors
HAND LABEL SHIT.
might have to empty database at the server and local before scraping new batch
id 4869,5266 is not english




Trivial to do list: 
Log errors from scraper D:

Components:
- should actually time how long the isbn calling takes on avg
-

- preprocess data (clean and assign genre) and store in separate table BOOK(id, title, genre, )
-


- ensure all english
- preprocess data
- not call gbook if no cover
- make sure keys are the same!


first 5000: range(1250107814, 1250740000):

Book Cover Dataset
https://github.com/uchidalab/book-dataset

Book_cover and title:
http://cs229.stanford.edu/proj2015/127_report.pdf

Delete rows with dupe col value book_title
DELETE t1.*
FROM
  book t1 INNER JOIN (SELECT book_title
                           FROM book
                           GROUP BY book_title
                           HAVING COUNT(*)>1) t2
  ON t1.book_title = t2.book_title;

DAtabase commands:
mysql -u root -p
SELECT * INTO OUTFILE '/var/lib/mysql-files/run2.csv' FIELDS TERMINATED BY '\t' OPTIONALLY ENCLOSED BY '"' LINES TERMINATED BY '\n' FROM book;
DEC 2018: SELECT *
FROM book
INTO OUTFILE '/var/lib/mysql-files/dec2018.csv'
FIELDS TERMINATED BY '\t'
ENCLOSED BY '"'
LINES TERMINATED BY '\n';
sudo apt-get install python-pip python-dev libmysqlclient-dev



sudo systemctl status mysql
pip3 install configparser
ALTER TABLE `book` ADD COLUMN `id` INT AUTO_INCREMENT UNIQUE FIRST;
ALTER TABLE book MODIFY COLUMN ID int NOT NULL AUTO_INCREMENT PRIMARY KEY (ID);
CREATE TABLE book ( book_title varchar(120), authors varchar(60), cover varchar(100), pub_year int, src varchar(2), genre varchar(20), description varchar(1500));
https://stackoverflow.com/questions/28973453/mysql2error-incorrect-string-value-xe2-x80-xa8-x09
#genre-classification

from local machine
cd /var/lib/mysql-files/
sudo scp root@128.199.109.98:/root/genre-classification/90k.csv .
https://www.itworld.com/article/2833078/it-management/3-ways-to-import-and-export-a-mysql-database.html
https://stackoverflow.com/questions/41645309/mysql-error-access-denied-for-user-rootlocalhost
https://stackoverflow.com/questions/40942061/changing-version-of-python-when-using-nohup
python3.6 -m pip install -r requirements.txt
https://stackoverflow.com/questions/1673530/error-2003-hy000-cant-connect-to-mysql-server-on-127-0-0-1-111


45734
remove books wirth desc == NaN
Index(['id', 'book_title', 'authors', 'pub_year', 'genre', 'description',
       'desc_word_count'],
      dtype='object')
39222 -> 6000 books have no descriptions




total = df_train.isnull().sum().sort_values(ascending=False)
percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)


https://www.kaggle.com/dejavu23/house-prices-eda-to-ml-beginner
def get_best_score(grid):
    
    best_score = np.sqrt(-grid.best_score_)
    print(best_score)    
    print(grid.best_params_)
    print(grid.best_estimator_)
    
    return best_score

def print_cols_large_corr(df, nr_c, targ) :
    corr = df.corr()
    corr_abs = corr.abs()
    print (corr_abs.nlargest(nr_c, targ)[targ])

def plot_corr_matrix(df, nr_c, targ) :
    
    corr = df.corr()
    corr_abs = corr.abs()
    cols = corr_abs.nlargest(nr_c, targ)[targ].index
    cm = np.corrcoef(df[cols].values.T)

    plt.figure(figsize=(nr_c/1.5, nr_c/1.5))
    sns.set(font_scale=1.25)
    sns.heatmap(cm, linewidths=1.5, annot=True, square=True, 
                fmt='.2f', annot_kws={'size': 10}, 
                yticklabels=cols.values, xticklabels=cols.values
               )
    plt.show()

all_data = pd.concat((df_train[feats], df_test[feats]))
https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/
# create a count vectorizer object 
count_vect = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}')
count_vect.fit(trainDF['text'])

# transform the training and validation data using count vectorizer object
xtrain_count =  count_vect.transform(train_x)
xvalid_count =  count_vect.transform(valid_x)

# word level tf-idf
tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_features=5000)
tfidf_vect.fit(trainDF['text'])
xtrain_tfidf =  tfidf_vect.transform(train_x)
xvalid_tfidf =  tfidf_vect.transform(valid_x)

Glove, FastText, and Word2Vec.

# load the pre-trained word-embedding vectors 
embeddings_index = {}
for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):
    values = line.split()
    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')

# create a tokenizer 
token = text.Tokenizer()
token.fit_on_texts(trainDF['text'])
word_index = token.word_index

# convert text to sequence of tokens and pad them to ensure equal length vectors 
train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)
valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)

# create token-embedding mapping
embedding_matrix = numpy.zeros((len(word_index) + 1, 300))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
