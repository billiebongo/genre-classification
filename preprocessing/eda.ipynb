{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c4bb5e640b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\\\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "\n",
    "import pickle \n",
    "#import mglearn\n",
    "import time\n",
    "\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from clean_data import clean_description as clean_des\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../preprocessing/data.csv', sep='\\t')\n",
    "\n",
    "#df.to_csv(\"output.csv\", index=False, sep = \"\\t\")\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "#shld change all scifi to science-fiction\n",
    "df.columns\n",
    "print(type(df.head()))\n",
    "df.groupby('genre').describe()\n",
    "\n",
    "print(df[\"genre\"].unique())\n",
    "\n",
    "\n",
    "print(df.groupby(['genre']).count())\n",
    "GENRE_LIST = [\"children\", \"fantasy\", \"young-adult\", \"self-help\", \"religion\", \"math\", \"historical\", \"science\", \n",
    "              \n",
    "              \"comics\", \"travel\", \"horror\", \"economics\", \"scifi\", \"history\", \\\n",
    "              \"business\", \"religion\", \"romance\",  \"philosophy\", \"humour\", \"finance\",\n",
    "              \"mystery\", \"thriller\", \"chick-lit\",  \"biography\", \"politics\",  \"art\", \"cookbook\",\n",
    "              \"drama\", \"adventure\"]#,  \"fiction\", \"non-fiction\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df['description'].isnull().sum())\n",
    "df = df[df['genre'].isin(GENRE_LIST)]\n",
    "\n",
    "\n",
    "df['count_word']=df[\"description\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "\n",
    "df['count_word'].loc[df['count_word']>50] = 50\n",
    "print(df['count_word'].mean())\n",
    "\n",
    "z=df.groupby(['genre'])[\"count_word\"].mean()\n",
    "z\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "#X_train_counts = count_vect.fit(df['description'].values.astype('U'))\n",
    "#print(X_train_counts.get_feature_names())\n",
    "#X_train_counts= count_vect.transform(df['description'].values.astype('U'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(smooth_idf=False, min_df=2, max_df=0.8,sublinear_tf=False, norm=None, analyzer='word')\n",
    "\n",
    "df['description']=(df['description'].apply(clean_des))\n",
    "print(len(df['description'].values))\n",
    "#df['book_title']=(df['book_title'].apply(clean_description))\n",
    "\n",
    "print(df.shape)\n",
    "df = df[df.description!=\"\"]\n",
    "df = df[df.description!=np.nan]\n",
    "print(df.shape)\n",
    "\n",
    "txt_fitted=tf.fit(df['description'].values.astype('U'))\n",
    "\n",
    "txt_transformed = txt_fitted.transform(df['description'].values.astype('U'))\n",
    "\n",
    "\n",
    "idf = tf.idf_\n",
    "\n",
    "#print(dict(zip(txt_fitted.get_feature_names(), idf)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_names = np.array(tf.get_feature_names())\n",
    "sorted_by_idf = np.argsort(tf.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[:3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['description']\n",
    "y = df['genre']  \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.1, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Instantiate the vectorizer\n",
    "word_vectorizer = TfidfVectorizer(min_df=2, max_df=0.8,\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=300000)\n",
    "# fit and transform on it the training features\n",
    "print(\"OK!\")\n",
    "word_vectorizer.fit(X_train)\n",
    "X_train_word_features = word_vectorizer.transform(X_train)\n",
    "\n",
    "#transform the test features to sparse matrix\n",
    "test_features = word_vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "classifier = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n",
    "print(\"done w gridsearch2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hel2p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"begin3\")\n",
    "best_model=classifier.fit(X_train_word_features, y_train)\n",
    "print(\"begin3\")\n",
    "y_pred = classifier.predict(test_features)\n",
    "y_pred_prob = classifier.predict_proba(test_features)[:, 1]\n",
    "print(\"test!!!!!!!!!!!!!!!!\")\n",
    "cv_score = np.mean(cross_val_score(classifier, X_train_word_features, y_train,\n",
    "                                   cv=5,scoring='accuracy'))\n",
    "print('CV Accuracy score is {}'.format( cv_score))\n",
    "print(\"gridsearchCV done\")\n",
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best\")\n",
    "print('Best pen:', best_model.best_estimator_.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "print(len(X_test))\n",
    "print(type(y_test))\n",
    "print(type(y_pred))\n",
    "\n",
    "for i in range(3,100):\n",
    "    print(X_test.iloc[i])\n",
    "    print(\"ground truth: {}\".format(str(y_test.iloc[i])))\n",
    "    print(\"predict: {}\".format(str(y_pred[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
